{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d662870",
   "metadata": {},
   "source": [
    "Instalamos las librerias necesarias (Python 3.13.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f39314da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai-whisper in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (20250625)\n",
      "Requirement already satisfied: librosa in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (0.11.0)\n",
      "Requirement already satisfied: pydub in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (0.25.1)\n",
      "Requirement already satisfied: scikit-learn in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (1.7.2)\n",
      "Requirement already satisfied: speechrecognition in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (3.14.3)\n",
      "Requirement already satisfied: ipykernel in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (6.30.1)\n",
      "Requirement already satisfied: more-itertools in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from openai-whisper) (10.8.0)\n",
      "Requirement already satisfied: numba in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from openai-whisper) (0.62.1)\n",
      "Requirement already satisfied: numpy in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from openai-whisper) (2.3.3)\n",
      "Requirement already satisfied: tiktoken in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from openai-whisper) (0.11.0)\n",
      "Requirement already satisfied: torch in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from openai-whisper) (2.8.0)\n",
      "Requirement already satisfied: tqdm in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from openai-whisper) (4.67.1)\n",
      "Requirement already satisfied: triton>=2 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from openai-whisper) (3.4.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from librosa) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.0 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from librosa) (1.5.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from librosa) (5.2.1)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from librosa) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from librosa) (1.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from librosa) (4.15.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from librosa) (1.1.1)\n",
      "Requirement already satisfied: standard-aifc in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from librosa) (3.13.0)\n",
      "Requirement already satisfied: standard-sunau in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from librosa) (3.13.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: audioop-lts in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from speechrecognition) (0.2.2)\n",
      "Requirement already satisfied: comm>=0.1.1 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from ipykernel) (0.2.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from ipykernel) (1.8.17)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from ipykernel) (9.6.0)\n",
      "Requirement already satisfied: jupyter-client>=8.0.0 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from ipykernel) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from ipykernel) (5.8.1)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from ipykernel) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio>=1.4 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from ipykernel) (1.6.0)\n",
      "Requirement already satisfied: packaging>=22 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from ipykernel) (25.0)\n",
      "Requirement already satisfied: psutil>=5.7 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from ipykernel) (7.1.0)\n",
      "Requirement already satisfied: pyzmq>=25 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from ipykernel) (27.1.0)\n",
      "Requirement already satisfied: tornado>=6.2 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from ipykernel) (6.5.2)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from ipykernel) (5.14.3)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from ipython>=7.23.1->ipykernel) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from ipython>=7.23.1->ipykernel) (0.19.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from ipython>=7.23.1->ipykernel) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from ipython>=7.23.1->ipykernel) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from ipython>=7.23.1->ipykernel) (2.19.2)\n",
      "Requirement already satisfied: stack_data in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from ipython>=7.23.1->ipykernel) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel) (0.2.14)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from jupyter-client>=8.0.0->ipykernel) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (4.4.0)\n",
      "Requirement already satisfied: llvmlite<0.46,>=0.45.0dev0 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from numba->openai-whisper) (0.45.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel) (0.7.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from pooch>=1.1->librosa) (2.32.5)\n",
      "Requirement already satisfied: six>=1.5 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from python-dateutil>=2.8.2->jupyter-client>=8.0.0->ipykernel) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.8.3)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from soundfile>=0.12.1->librosa) (2.0.0)\n",
      "Requirement already satisfied: pycparser in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.23)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from triton>=2->openai-whisper) (78.1.1)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from stack_data->ipython>=7.23.1->ipykernel) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from stack_data->ipython>=7.23.1->ipykernel) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from stack_data->ipython>=7.23.1->ipykernel) (0.2.3)\n",
      "Requirement already satisfied: standard-chunk in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from standard-aifc->librosa) (3.13.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from tiktoken->openai-whisper) (2025.9.18)\n",
      "Requirement already satisfied: filelock in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from torch->openai-whisper) (3.19.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from torch->openai-whisper) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from torch->openai-whisper) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from torch->openai-whisper) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from torch->openai-whisper) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from torch->openai-whisper) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from torch->openai-whisper) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from torch->openai-whisper) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from torch->openai-whisper) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from torch->openai-whisper) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from torch->openai-whisper) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from torch->openai-whisper) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from torch->openai-whisper) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from torch->openai-whisper) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from torch->openai-whisper) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from torch->openai-whisper) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from torch->openai-whisper) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from torch->openai-whisper) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from torch->openai-whisper) (1.13.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from sympy>=1.13.3->torch->openai-whisper) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/hm/anaconda3/envs/audio2text/lib/python3.13/site-packages (from jinja2->torch->openai-whisper) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai-whisper librosa pydub scikit-learn speechrecognition ipykernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd0a6fc",
   "metadata": {},
   "source": [
    "Importamos las librerias necesarias y creamos las funciones que necesitamos para el proyecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cc4a7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INICIANDO PROCESAMIENTO MASIVO DE AUDIOS\n",
      "==================================================\n",
      "Se encontraron 1 archivos de audio para procesar\n",
      "============================================================\n",
      "\n",
      "[1/1] ejemplo.mp3\n",
      "----------------------------------------\n",
      "Procesando: ejemplo.mp3\n",
      "  Detectando número de hablantes...\n",
      "  Número de hablantes detectados: 2\n",
      "  Realizando diarización avanzada...\n",
      "  Archivo temporal eliminado\n",
      "  Transcripción guardada en: Transcripciones/ejemplo_transcripcion.txt\n",
      "  --- ESTADÍSTICAS ---\n",
      "  Persona_02: 4 segmentos, 20.0s, 17 palabras\n",
      "  Persona_01: 4 segmentos, 101.0s, 178 palabras\n",
      "----------------------------------------\n",
      "\n",
      "============================================================\n",
      "PROCESAMIENTO COMPLETADO\n",
      "Archivos procesados exitosamente: 1/1\n",
      "Archivos con errores: 0\n",
      "Transcripciones guardadas en: /home/hm/Documents/Github/audio-to-text-conversion/audio-to-text-conversion/Transcripciones\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "import librosa\n",
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "import os\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "import glob\n",
    "from datetime import datetime\n",
    "\n",
    "def convert_mp3_to_wav(mp3_path):\n",
    "    \"\"\"Convierte MP3 a WAV manteniendo la calidad\"\"\"\n",
    "    wav_path = mp3_path.replace('.mp3', '_converted.wav')\n",
    "    audio = AudioSegment.from_mp3(mp3_path)\n",
    "    # Convertir a mono y 16kHz para mejor procesamiento\n",
    "    audio = audio.set_channels(1).set_frame_rate(16000)\n",
    "    audio.export(wav_path, format='wav')\n",
    "    return wav_path\n",
    "\n",
    "def detect_number_of_speakers(audio_path, max_speakers=4):\n",
    "    \"\"\"\n",
    "    Detecta automáticamente el número de hablantes usando análisis espectral\n",
    "    \"\"\"\n",
    "    print(\"  Detectando número de hablantes...\")\n",
    "    \n",
    "    # Cargar audio\n",
    "    y, sr = librosa.load(audio_path, sr=16000)\n",
    "    \n",
    "    # Extraer características para clustering\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20, hop_length=512)\n",
    "    mfccs = mfccs.T\n",
    "    \n",
    "    # Probar diferentes números de clusters\n",
    "    best_score = -1\n",
    "    best_n = 2  # Default a 2 hablantes\n",
    "    \n",
    "    for n in range(2, min(max_speakers + 1, len(mfccs))):\n",
    "        try:\n",
    "            clustering = AgglomerativeClustering(n_clusters=n)\n",
    "            labels = clustering.fit_predict(mfccs[:1000])  # Usar subset para velocidad\n",
    "            \n",
    "            # Calcular score de silueta (métrica de calidad del clustering)\n",
    "            if len(np.unique(labels)) > 1:\n",
    "                score = silhouette_score(mfccs[:1000], labels)\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_n = n\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    print(f\"  Número de hablantes detectados: {best_n}\")\n",
    "    return best_n\n",
    "\n",
    "def extract_voice_embeddings(audio_path, segments):\n",
    "    \"\"\"\n",
    "    Extrae embeddings de voz para cada segmento usando características MFCC avanzadas\n",
    "    \"\"\"\n",
    "    y, sr = librosa.load(audio_path, sr=16000)\n",
    "    \n",
    "    embeddings = []\n",
    "    valid_segments = []\n",
    "    \n",
    "    for segment in segments:\n",
    "        start_sample = int(segment[\"start\"] * sr)\n",
    "        end_sample = int(segment[\"end\"] * sr)\n",
    "        \n",
    "        # Asegurar que los índices estén dentro del rango\n",
    "        if start_sample < len(y) and end_sample <= len(y) and (end_sample - start_sample) > 512:\n",
    "            segment_audio = y[start_sample:end_sample]\n",
    "            \n",
    "            # Extraer características MFCC avanzadas\n",
    "            mfcc = librosa.feature.mfcc(\n",
    "                y=segment_audio, \n",
    "                sr=sr, \n",
    "                n_mfcc=20,\n",
    "                n_fft=2048,\n",
    "                hop_length=512\n",
    "            )\n",
    "            \n",
    "            # Calcular estadísticas de las características\n",
    "            mfcc_mean = np.mean(mfcc, axis=1)\n",
    "            mfcc_std = np.std(mfcc, axis=1)\n",
    "            \n",
    "            # Combinar en un embedding\n",
    "            embedding = np.concatenate([mfcc_mean, mfcc_std])\n",
    "            embeddings.append(embedding)\n",
    "            valid_segments.append(segment)\n",
    "    \n",
    "    return np.array(embeddings), valid_segments\n",
    "\n",
    "def advanced_speaker_diarization(audio_path, segments, num_speakers):\n",
    "    \"\"\"\n",
    "    Diarización avanzada usando embeddings de voz y clustering\n",
    "    \"\"\"\n",
    "    print(\"  Realizando diarización avanzada...\")\n",
    "    \n",
    "    # Extraer embeddings\n",
    "    embeddings, valid_segments = extract_voice_embeddings(audio_path, segments)\n",
    "    \n",
    "    if len(embeddings) < 2:\n",
    "        print(\"  No hay suficientes segmentos para diarización\")\n",
    "        for segment in segments:\n",
    "            segment[\"speaker\"] = \"Persona_01\"\n",
    "        return segments\n",
    "    \n",
    "    # Clustering con número de hablantes detectado\n",
    "    clustering = AgglomerativeClustering(n_clusters=min(num_speakers, len(embeddings)))\n",
    "    speaker_labels = clustering.fit_predict(embeddings)\n",
    "    \n",
    "    # Asignar etiquetas a los segmentos válidos\n",
    "    for i, segment in enumerate(valid_segments):\n",
    "        segment[\"speaker\"] = f\"Persona_{speaker_labels[i] + 1:02d}\"\n",
    "    \n",
    "    # Para segmentos no válidos, asignar basado en proximidad temporal\n",
    "    last_valid_speaker = \"Persona_01\"\n",
    "    for segment in segments:\n",
    "        if \"speaker\" not in segment:\n",
    "            segment[\"speaker\"] = last_valid_speaker\n",
    "        else:\n",
    "            last_valid_speaker = segment[\"speaker\"]\n",
    "    \n",
    "    return segments\n",
    "\n",
    "def group_consecutive_segments(segments, pause_threshold=2.0):\n",
    "    \"\"\"\n",
    "    Agrupa segmentos consecutivos del mismo hablante y añade \"...\" para pausas\n",
    "    \"\"\"\n",
    "    if not segments:\n",
    "        return []\n",
    "    \n",
    "    grouped_segments = []\n",
    "    current_speaker = segments[0][\"speaker\"]\n",
    "    current_text = segments[0][\"text\"].strip()\n",
    "    current_start = segments[0][\"start\"]\n",
    "    current_end = segments[0][\"end\"]\n",
    "    \n",
    "    for i in range(1, len(segments)):\n",
    "        current_segment = segments[i]\n",
    "        prev_segment = segments[i-1]\n",
    "        \n",
    "        # Calcular pausa entre segmentos\n",
    "        pause_duration = current_segment[\"start\"] - prev_segment[\"end\"]\n",
    "        \n",
    "        # Si es el mismo hablante\n",
    "        if current_segment[\"speaker\"] == current_speaker:\n",
    "            # Si hay una pausa significativa, añadir \"...\"\n",
    "            if pause_duration > pause_threshold:\n",
    "                current_text += \" ... \" + current_segment[\"text\"].strip()\n",
    "            else:\n",
    "                current_text += \" \" + current_segment[\"text\"].strip()\n",
    "            \n",
    "            # Actualizar el tiempo final\n",
    "            current_end = current_segment[\"end\"]\n",
    "        \n",
    "        # Si cambia de hablante\n",
    "        else:\n",
    "            # Guardar el segmento agrupado anterior\n",
    "            grouped_segments.append({\n",
    "                \"speaker\": current_speaker,\n",
    "                \"text\": current_text,\n",
    "                \"start\": current_start,\n",
    "                \"end\": current_end\n",
    "            })\n",
    "            \n",
    "            # Comenzar nuevo grupo\n",
    "            current_speaker = current_segment[\"speaker\"]\n",
    "            current_text = current_segment[\"text\"].strip()\n",
    "            current_start = current_segment[\"start\"]\n",
    "            current_end = current_segment[\"end\"]\n",
    "    \n",
    "    # Añadir el último segmento\n",
    "    grouped_segments.append({\n",
    "        \"speaker\": current_speaker,\n",
    "        \"text\": current_text,\n",
    "        \"start\": current_start,\n",
    "        \"end\": current_end\n",
    "    })\n",
    "    \n",
    "    return grouped_segments\n",
    "\n",
    "def process_single_audio(audio_path):\n",
    "    \"\"\"Procesa un único archivo de audio y retorna los segmentos transcritos\"\"\"\n",
    "    temp_wav_path = None\n",
    "    \n",
    "    try:\n",
    "        print(f\"Procesando: {os.path.basename(audio_path)}\")\n",
    "        \n",
    "        # 1. Convertir a WAV si es MP3\n",
    "        if audio_path.endswith('.mp3'):\n",
    "            audio_path = convert_mp3_to_wav(audio_path)\n",
    "            temp_wav_path = audio_path\n",
    "        \n",
    "        # 2. Detectar número de hablantes\n",
    "        num_speakers = detect_number_of_speakers(audio_path)\n",
    "        \n",
    "        # 3. Cargar modelo de Whisper\n",
    "        model = whisper.load_model(\"base\")\n",
    "        \n",
    "        # 4. Transcribir el audio completo\n",
    "        result = model.transcribe(audio_path)\n",
    "        \n",
    "        # 5. Filtrar segmentos muy cortos (probablemente ruido)\n",
    "        segments = [\n",
    "            segment for segment in result[\"segments\"] \n",
    "            if segment[\"end\"] - segment[\"start\"] > 0.5  # Mínimo 0.5 segundos\n",
    "        ]\n",
    "        \n",
    "        # 6. Diarización avanzada\n",
    "        segments = advanced_speaker_diarization(audio_path, segments, num_speakers)\n",
    "        \n",
    "        # 7. Agrupar segmentos consecutivos del mismo hablante\n",
    "        grouped_segments = group_consecutive_segments(segments)\n",
    "        \n",
    "        return grouped_segments\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error procesando {audio_path}: {str(e)}\")\n",
    "        return None\n",
    "        \n",
    "    finally:\n",
    "        # 8. Limpiar archivo temporal si existe\n",
    "        if temp_wav_path and os.path.exists(temp_wav_path):\n",
    "            os.remove(temp_wav_path)\n",
    "            print(f\"  Archivo temporal eliminado\")\n",
    "\n",
    "def save_transcription(segments, output_path, audio_filename):\n",
    "    \"\"\"Guarda la transcripción en formato legible con segmentos agrupados\"\"\"\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"TRANSCRIPCIÓN: {audio_filename}\\n\")\n",
    "        f.write(f\"FECHA DE PROCESAMIENTO: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(\"=\" * 70 + \"\\n\\n\")\n",
    "        \n",
    "        for segment in segments:\n",
    "            f.write(f\"{segment['speaker']} ({segment['start']:.1f}s - {segment['end']:.1f}s):\\n\")\n",
    "            f.write(f\"  {segment['text']}\\n\\n\")\n",
    "    \n",
    "    print(f\"  Transcripción guardada en: {output_path}\")\n",
    "\n",
    "def analyze_speaker_statistics(segments):\n",
    "    \"\"\"Analiza estadísticas de los hablantes\"\"\"\n",
    "    speakers = {}\n",
    "    \n",
    "    for segment in segments:\n",
    "        speaker = segment[\"speaker\"]\n",
    "        duration = segment[\"end\"] - segment[\"start\"]\n",
    "        \n",
    "        if speaker not in speakers:\n",
    "            speakers[speaker] = {\n",
    "                \"segments\": 0,\n",
    "                \"total_duration\": 0,\n",
    "                \"word_count\": 0\n",
    "            }\n",
    "        \n",
    "        speakers[speaker][\"segments\"] += 1\n",
    "        speakers[speaker][\"total_duration\"] += duration\n",
    "        speakers[speaker][\"word_count\"] += len(segment[\"text\"].split())\n",
    "    \n",
    "    return speakers\n",
    "\n",
    "def process_all_audios(input_folder=\"audios\", output_folder=\"transcripcion\"):\n",
    "    \"\"\"\n",
    "    Procesa todos los archivos de audio en la carpeta de entrada\n",
    "    y guarda las transcripciones en la carpeta de salida\n",
    "    \"\"\"\n",
    "    # Crear carpeta de salida si no existe\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "        print(f\"Carpeta '{output_folder}' creada\")\n",
    "    \n",
    "    # Buscar archivos de audio\n",
    "    audio_extensions = ['*.mp3', '*.wav', '*.m4a', '*.flac']\n",
    "    audio_files = []\n",
    "    \n",
    "    for extension in audio_extensions:\n",
    "        audio_files.extend(glob.glob(os.path.join(input_folder, extension)))\n",
    "    \n",
    "    if not audio_files:\n",
    "        print(f\"No se encontraron archivos de audio en la carpeta '{input_folder}'\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Se encontraron {len(audio_files)} archivos de audio para procesar\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Estadísticas generales\n",
    "    total_files = len(audio_files)\n",
    "    processed_files = 0\n",
    "    failed_files = 0\n",
    "    \n",
    "    # Procesar cada archivo\n",
    "    for audio_path in audio_files:\n",
    "        audio_filename = os.path.basename(audio_path)\n",
    "        output_filename = os.path.splitext(audio_filename)[0] + \"_transcripcion.txt\"\n",
    "        output_path = os.path.join(output_folder, output_filename)\n",
    "        \n",
    "        print(f\"\\n[{processed_files + 1}/{total_files}] {audio_filename}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Procesar el audio\n",
    "        segments = process_single_audio(audio_path)\n",
    "        \n",
    "        if segments:\n",
    "            # Guardar transcripción\n",
    "            save_transcription(segments, output_path, audio_filename)\n",
    "            \n",
    "            # Mostrar estadísticas del archivo\n",
    "            speaker_stats = analyze_speaker_statistics(segments)\n",
    "            print(\"  --- ESTADÍSTICAS ---\")\n",
    "            for speaker, stats in speaker_stats.items():\n",
    "                print(f\"  {speaker}: {stats['segments']} segmentos, \"\n",
    "                      f\"{stats['total_duration']:.1f}s, {stats['word_count']} palabras\")\n",
    "            \n",
    "            processed_files += 1\n",
    "        else:\n",
    "            print(f\"  ❌ Error al procesar {audio_filename}\")\n",
    "            failed_files += 1\n",
    "        \n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    # Resumen final\n",
    "    print(f\"\\n\" + \"=\" * 60)\n",
    "    print(\"PROCESAMIENTO COMPLETADO\")\n",
    "    print(f\"Archivos procesados exitosamente: {processed_files}/{total_files}\")\n",
    "    print(f\"Archivos con errores: {failed_files}\")\n",
    "    \n",
    "    if processed_files > 0:\n",
    "        print(f\"Transcripciones guardadas en: {os.path.abspath(output_folder)}\")\n",
    "\n",
    "# Script principal\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuración de carpetas\n",
    "    INPUT_FOLDER = \"Audios\"\n",
    "    OUTPUT_FOLDER = \"Transcripciones\"\n",
    "    \n",
    "    try:\n",
    "        print(\"INICIANDO PROCESAMIENTO MASIVO DE AUDIOS\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Verificar que existe la carpeta de audios\n",
    "        if not os.path.exists(INPUT_FOLDER):\n",
    "            print(f\"Error: No existe la carpeta '{INPUT_FOLDER}'\")\n",
    "            print(\"Creando estructura de carpetas...\")\n",
    "            os.makedirs(INPUT_FOLDER)\n",
    "            print(f\"Por favor, coloca tus archivos de audio en la carpeta '{INPUT_FOLDER}' y ejecuta el script nuevamente\")\n",
    "        else:\n",
    "            # Procesar todos los audios\n",
    "            process_all_audios(INPUT_FOLDER, OUTPUT_FOLDER)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error durante el procesamiento: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a34b410",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio2text",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
